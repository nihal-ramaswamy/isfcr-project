# -*- coding: utf-8 -*-
"""TRAINING

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/102iLcVkV7Fd4R_J7Grcto6Jb2eCWbKZu
"""

#import libraries
import numpy as np
import pandas as pd 
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from sklearn.pipeline import Pipeline
import pickle
from sklearn.externals import joblib

#data preprocessing
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn import *

#NLP tools
import re
import nltk
nltk.download('stopwords')
nltk.download('rslp')
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from sklearn.feature_extraction.text import CountVectorizer

#train split and fit models
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from nltk.tokenize import TweetTokenizer

#model selection
from sklearn.metrics import confusion_matrix, accuracy_score, plot_confusion_matrix, classification_report
import os
import pickle

path_to_data = '/content/drive/MyDrive/ISFCR Project folder/labeled_data.csv'
PATH_TO_NLTK_MODEL = '/content/drive/MyDrive/ISFCR Project folder/nltk.pickle'
PATH_TO_SCIKIT_MODEL = '/content/drive/MyDrive/ISFCR Project folder/model.pkl'

dataset = pd.read_csv(path_to_data)

#dt_transformed -> relevent columns for analysis
dt_transformed = dataset[['class', 'tweet']]
y = (dt_transformed.iloc[:, :-1].values).ravel()

df_train, df_test = train_test_split(dt_transformed, test_size = 0.10, random_state = 42, stratify=dt_transformed['class'])

#Preprocessing 
def preprocessing(data):
    stemmer = nltk.stem.RSLPStemmer()
    all_stopwords = stopwords.words('english')
    all_stopwords.remove('not')
    corpus = []
    for words in data:
      review = re.sub(r"@[A-Za-z0-9_]+", " ", words)
      review = re.sub('RT', ' ', review)
      review = re.sub(r"https?://[A-Za-z0-9./]+", " ", review)
      review = re.sub(r"https?", " ", review)
      review = re.sub('[^a-zA-Z]', ' ', review)
      review = review.lower()
      review = review.split()
      ps = PorterStemmer()
      review = [ps.stem(word) for word in review if not word in set(all_stopwords) if len(word) > 2]
      review = ' '.join(review)
      corpus.append(review)

    return np.array(corpus)

corpus = preprocessing(df_train['tweet'].values)
#further split into training and validation 
c_train, c_vad, y_train, y_vad = train_test_split(corpus, df_train['class'], test_size = 0.10, random_state = 42, stratify=df_train['class'])

#Tokenization for extracting features
def tokenize(c_train, c_vad):
    tweet_tokenizer = TweetTokenizer() 
    vectorizer = CountVectorizer(analyzer="word", tokenizer=tweet_tokenizer.tokenize, max_features = 1000)
    X_train = vectorizer.fit_transform(c_train).toarray()
    X_vad = vectorizer.transform(c_vad).toarray()
    return X_train, X_vad, vectorizer

df_hate = df_train[df_train['class'] == 0]
df_off = df_train[df_train['class'] == 1]
df_none = df_train[df_train['class'] == 2]

n_off, n_none, n_hate = df_train['class'].value_counts()

#Oversampling to remove bias
df_hate_over = df_hate.sample(n_off, replace=True, random_state=0)
df_none_over = df_none.sample(n_off, replace=True, random_state=0)
df_over = pd.concat([df_off, df_hate_over, df_none_over], axis=0)

print('Random over-sampling:')
print(df_over['class'].value_counts())

corpus_over = preprocessing(df_over['tweet'].values)
# training and validation
c_train, c_vad, y_train, y_vad = train_test_split(corpus_over, df_over['class'], test_size = 0.10, random_state = 42, stratify=df_over['class'])

X_train, X_vad, vec3 = tokenize(c_train, c_vad)

#Logistic regression after oversampling 
model = LogisticRegression(multi_class='ovr', solver='liblinear', random_state = 0)
model.fit(X_train, y_train.ravel())
y_pred = model.predict(X_vad)

target_names = ['class 0', 'class 1', 'class 2']
print(classification_report(y_vad, y_pred, target_names=target_names))

"""**SAVING MODELS**"""

save_classifier = open(PATH_TO_NLTK_MODEL,"wb")
pickle.dump(vec3, save_classifier)
save_classifier.close()

joblib.dump(model, PATH_TO_SCIKIT_MODEL)